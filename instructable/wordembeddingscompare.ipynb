{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9647803e-3303-4a09-83cb-86fa4e4cca0a",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.13.2' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "folder_path = \"final\"\n",
    "combined_data = []\n",
    "\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".json\"):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "                if isinstance(data, list):  \n",
    "                    combined_data.extend(data)\n",
    "                elif isinstance(data, dict):\n",
    "                    combined_data.append(data)\n",
    "        except (json.JSONDecodeError, UnicodeDecodeError) as e:\n",
    "            print(f\"Skipping {filename} due to error: {e}\")\n",
    "\n",
    "\n",
    "output_file = \"merged.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(combined_data, f, indent=4)\n",
    "\n",
    "print(f\"✅ Merged {len(combined_data)} JSON entries into {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa05a75-4b0d-4fe7-aff7-6e9db00c54f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence_transformers in c:\\users\\sathya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.4.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\sathya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sentence_transformers) (4.49.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\sathya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sentence_transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\sathya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sentence_transformers) (2.6.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\sathya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sentence_transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\sathya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sentence_transformers) (1.15.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\sathya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sentence_transformers) (0.29.1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\sathya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sentence_transformers) (11.1.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\sathya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\sathya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\sathya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sathya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\sathya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\sathya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\sathya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sathya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (3.1.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sathya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\sathya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\sathya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\sathya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm->sentence_transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\sathya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2.1.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\sathya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\sathya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\sathya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.5.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\sathya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\sathya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sathya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sathya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sathya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sathya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sathya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2025.1.31)\n",
      "Processed 748 records. Output saved to combined_data_with_embeddings.json.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "folder_path = \"final\"\n",
    "combined_data = []\n",
    "\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".json\"):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "                \n",
    "                if isinstance(data, list):  \n",
    "                    combined_data.extend(data)\n",
    "                elif isinstance(data, dict):\n",
    "                    combined_data.append(data)\n",
    "        except (json.JSONDecodeError, UnicodeDecodeError) as e:\n",
    "            print(f\"Skipping {filename} due to error: {e}\")\n",
    "\n",
    "\n",
    "for item in combined_data:\n",
    "    if \"materials_required\" in item and isinstance(item[\"materials_required\"], list):\n",
    "        materials_text = \", \".join(item[\"materials_required\"])\n",
    "        embedding = model.encode(materials_text).tolist() \n",
    "        item[\"embedding\"] = embedding \n",
    "\n",
    "\n",
    "output_path = \"combined_data_with_embeddings.json\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(combined_data, f, indent=4)\n",
    "\n",
    "print(f\"Processed {len(combined_data)} records. Output saved to {output_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426987da-a2ff-4bfd-9965-d751f6d2f832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Match 1:\n",
      "Materials Required: ['Wood', 'Aluminum', 'Uni Joint', 'Bolts', 'Nails', 'Super glue with spray activator', 'Packing tape']\n",
      "Steps: ['Cut thin strips of wood for bent lamination using a table saw and soak them in water for about an hour to make them flexible.', 'While the wood is soaking, make small wedges on the table saw by tilting the mitre gauge to 10 degrees.', 'Bend the wet strips around a circular form, attaching small pieces of wood to secure the lamination with brad nails, leaving a small gap. Hammer the wedges into the gaps to apply pressure and eliminate gaps, then let it dry overnight.', 'Glue up the bent lamination, applying a lot of glue to all surfaces and covering the form with packing tape to prevent sticking. Use the wedges again to ensure good pressure and no gaps. Wipe off excess glue with a damp cloth.', 'Create the handle using a Uni Joint, drilling a hole near the end of a rod and cutting down the sides. Attach a small piece of wood with two holes at 90 degrees to connect the pieces, using nails, cutting them flush, and applying super glue with spray activator.', 'Cut and bend a piece of aluminum, attaching the handle to the small end with a bolt and two nuts. Adjust the handle position to achieve proper balance. Drill holes in the bottom of the lamination and insert bolts to create a counterweight until the balance feels right.']\n",
      "Similarity Score: 0.7756\n",
      "\n",
      "Match 2:\n",
      "Materials Required: ['Cardboard', 'Glue', 'Paint', 'Fibreglass (optional)']\n",
      "Steps: ['Print out the provided template.', 'Cut out all the template pieces.', 'Cut out the template pieces from cardboard.', 'Glue the edges and parts onto the front plate.', 'Paint the helmet. Optionally, fibreglass it for added durability and a smoother finish.']\n",
      "Similarity Score: 0.6988\n",
      "\n",
      "Match 3:\n",
      "Materials Required: ['Arduino Yún', \"2 servo's\", 'Buzzer', 'Printed circuit board', 'Laser cut wood (2 cowboys, 2 cowboy arms, 5 sides)', 'Glue']\n",
      "Steps: ['Set up the Arduino Yún and test the button mechanic using LEDs to determine the first press and block the other button.', \"Test the servo's to ensure they turn 90 degrees in opposite directions for each cowboy arm, programming the right cowboy's servo in reverse.\", 'Program the buzzer to produce a countdown with 4 beeps, triggering the game when the 4th beep sounds.', 'Laser cut the required wooden pieces: 2 cowboys (mirrored), 2 cowboy arms (mirrored), and 5 sides for the box with holes for buttons and cowboys.', \"Assemble the project by building the Arduino and breadboard, installing buttons, and positioning servo's inside the box.\", \"Glue the cowboys and attach the servo's to them, then glue on the arms and test the rotation of the servo's to ensure proper function.\"]\n",
      "Similarity Score: 0.6434\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "with open(\"combined_data_with_embeddings.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    combined_data = json.load(f)\n",
    "\n",
    "\n",
    "def find_similar_materials(user_materials, top_n=3):\n",
    "\n",
    "    user_text = \", \".join(user_materials)\n",
    "    user_embedding = model.encode(user_text).reshape(1, -1)\n",
    "\n",
    "    similarities = []\n",
    "    \n",
    "    for item in combined_data:\n",
    "        if \"embedding\" in item:\n",
    "            stored_embedding = np.array(item[\"embedding\"]).reshape(1, -1)\n",
    "            similarity_score = cosine_similarity(user_embedding, stored_embedding)[0][0]\n",
    "            similarities.append((item, similarity_score))\n",
    "\n",
    "\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    " \n",
    "    return similarities[:top_n]\n",
    "\n",
    "\n",
    "user_input_materials = [\"wood\", \"nails\", \"glue\"]\n",
    "\n",
    "\n",
    "top_matches = find_similar_materials(user_input_materials)\n",
    "\n",
    "# Print results\n",
    "for i, (match, score) in enumerate(top_matches, 1):\n",
    "    print(f\"\\nMatch {i}:\")\n",
    "    print(f\"Materials Required: {match['materials_required']}\")\n",
    "    print(f\"Steps: {match.get('steps', 'No steps provided')}\")\n",
    "    print(f\"Similarity Score: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108a6dc4-e910-48aa-be6b-af0fc6014d0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
